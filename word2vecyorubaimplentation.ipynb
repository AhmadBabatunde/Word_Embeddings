{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-31T11:13:44.621558Z","iopub.execute_input":"2024-08-31T11:13:44.621878Z","iopub.status.idle":"2024-08-31T11:13:44.991376Z","shell.execute_reply.started":"2024-08-31T11:13:44.621842Z","shell.execute_reply":"2024-08-31T11:13:44.990564Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install nltk\n!pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-08-31T11:18:06.479943Z","iopub.execute_input":"2024-08-31T11:18:06.480497Z","iopub.status.idle":"2024-08-31T11:18:33.654204Z","shell.execute_reply.started":"2024-08-31T11:18:06.480459Z","shell.execute_reply":"2024-08-31T11:18:33.653220Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')\n\nclass Word2Vec(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(Word2Vec, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear = nn.Linear(embedding_dim, vocab_size)\n    \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.linear(x)\n        return x\n\ndef preprocessing(corpus):\n    stop_words = set(stopwords.words('english'))\n    sentences = corpus.split(\".\")\n    training_data = []\n    for sentence in sentences:\n        sentence = sentence.strip().split()\n        sentence = [word.strip(string.punctuation).lower() for word in sentence if word.lower() not in stop_words]\n        training_data.append(sentence)\n    return training_data\n\ndef prepare_data_for_training(sentences):\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            if word not in vocab:\n                vocab[word] = len(vocab)\n    \n    X_train = []\n    y_train = []\n    for sentence in sentences:\n        for i, word in enumerate(sentence):\n            center_word = vocab[word]\n            context_words = []\n            for j in range(i-2, i+3):\n                if j != i and j >= 0 and j < len(sentence):\n                    context_words.append(vocab[sentence[j]])\n            \n            for context_word in context_words:\n                X_train.append(center_word)\n                y_train.append(context_word)\n    \n    return X_train, y_train, vocab\n\ndef train_model(model, X_train, y_train, epochs, device):\n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001)\n\n    for epoch in range(epochs):\n        total_loss = 0\n        for i in range(len(X_train)):\n            center_word = torch.tensor([X_train[i]], dtype=torch.long).to(device)\n            context_word = torch.tensor([y_train[i]], dtype=torch.long).to(device)\n\n            optimizer.zero_grad()\n            output = model(center_word)\n            loss = criterion(output, context_word)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X_train)}')\n\ndef predict(model, word, vocab, top_k, device):\n    if word not in vocab:\n        print(\"Word not found in dictionary\")\n        return []\n    \n    word_index = vocab[word]\n    word_tensor = torch.tensor([word_index], dtype=torch.long).to(device)\n    with torch.no_grad():\n        output = model(word_tensor).cpu().numpy()\n    \n    softmax_output = np.exp(output) / np.sum(np.exp(output))\n    top_k_indices = softmax_output.argsort()[0][-top_k:][::-1]\n    \n    index_to_word = {v: k for k, v in vocab.items()}\n    top_k_words = [index_to_word[i] for i in top_k_indices]\n    \n    return top_k_words\n\n# Example usage\ncorpus = \"Ayé yìí yípo orún káàkiri ayé. Òṣùpá náà yípo ayé lọ́títọ́. Òrùn ń ràn sí gbogbo ayé. Ilẹ̀ ń gbó ayé ní àpáta. Òjò ń ró sí gbogbo ilẹ̀. Ayé ń yípo sí ayé nínú àwọ̀n èdá.\"\nepochs = 1000\n\ntraining_data = preprocessing(corpus)\nX_train, y_train, vocab = prepare_data_for_training(training_data)\nvocab_size = len(vocab)\nembedding_dim = 10\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Word2Vec(vocab_size, embedding_dim)\ntrain_model(model, X_train, y_train, epochs, device)\n\nprint(predict(model, \"yípo\", vocab, 3, device))","metadata":{"execution":{"iopub.status.busy":"2024-08-31T11:24:43.438571Z","iopub.execute_input":"2024-08-31T11:24:43.439357Z","iopub.status.idle":"2024-08-31T11:26:27.932227Z","shell.execute_reply.started":"2024-08-31T11:24:43.439307Z","shell.execute_reply":"2024-08-31T11:26:27.931111Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nEpoch 1/1000, Loss: 3.114971248166902\nEpoch 2/1000, Loss: 3.0999887883663177\nEpoch 3/1000, Loss: 3.085448737655367\nEpoch 4/1000, Loss: 3.071343047278268\nEpoch 5/1000, Loss: 3.05766330233642\nEpoch 6/1000, Loss: 3.044400434408869\nEpoch 7/1000, Loss: 3.031544955713408\nEpoch 8/1000, Loss: 3.0190867483615875\nEpoch 9/1000, Loss: 3.007015194211687\nEpoch 10/1000, Loss: 2.9953192685331618\nEpoch 11/1000, Loss: 2.983987412282399\nEpoch 12/1000, Loss: 2.9730077236890793\nEpoch 13/1000, Loss: 2.962368062564305\nEpoch 14/1000, Loss: 2.952055988567216\nEpoch 15/1000, Loss: 2.9420589985592023\nEpoch 16/1000, Loss: 2.9323644808360507\nEpoch 17/1000, Loss: 2.922959883298193\nEpoch 18/1000, Loss: 2.9138327730553493\nEpoch 19/1000, Loss: 2.904970863035747\nEpoch 20/1000, Loss: 2.8963620981999805\nEpoch 21/1000, Loss: 2.8879946874720708\nEpoch 22/1000, Loss: 2.8798573708959987\nEpoch 23/1000, Loss: 2.8719390077250346\nEpoch 24/1000, Loss: 2.8642291330865453\nEpoch 25/1000, Loss: 2.856717507754053\nEpoch 26/1000, Loss: 2.8493945513452803\nEpoch 27/1000, Loss: 2.8422509942735945\nEpoch 28/1000, Loss: 2.835278124681541\nEpoch 29/1000, Loss: 2.8284677075488225\nEpoch 30/1000, Loss: 2.821811931473868\nEpoch 31/1000, Loss: 2.815303391643933\nEpoch 32/1000, Loss: 2.8089352026581764\nEpoch 33/1000, Loss: 2.8027008590953693\nEpoch 34/1000, Loss: 2.7965943217277527\nEpoch 35/1000, Loss: 2.790609732270241\nEpoch 36/1000, Loss: 2.784741817840508\nEpoch 37/1000, Loss: 2.778985468404634\nEpoch 38/1000, Loss: 2.7733360699244907\nEpoch 39/1000, Loss: 2.7677891169275557\nEpoch 40/1000, Loss: 2.7623405254312923\nEpoch 41/1000, Loss: 2.7569863849452565\nEpoch 42/1000, Loss: 2.7517230329769\nEpoch 43/1000, Loss: 2.7465470358729362\nEpoch 44/1000, Loss: 2.741455245230879\nEpoch 45/1000, Loss: 2.7364445669310435\nEpoch 46/1000, Loss: 2.7315121995551244\nEpoch 47/1000, Loss: 2.7266554225768362\nEpoch 48/1000, Loss: 2.721871741116047\nEpoch 49/1000, Loss: 2.717158690094948\nEpoch 50/1000, Loss: 2.7125140950083733\nEpoch 51/1000, Loss: 2.7079358026385307\nEpoch 52/1000, Loss: 2.7034217236297473\nEpoch 53/1000, Loss: 2.698970008109297\nEpoch 54/1000, Loss: 2.6945787827883447\nEpoch 55/1000, Loss: 2.6902463382908275\nEpoch 56/1000, Loss: 2.68597098333495\nEpoch 57/1000, Loss: 2.6817511447838376\nEpoch 58/1000, Loss: 2.677585358066218\nEpoch 59/1000, Loss: 2.673472129872867\nEpoch 60/1000, Loss: 2.6694101254854883\nEpoch 61/1000, Loss: 2.665398037859372\nEpoch 62/1000, Loss: 2.661434566336019\nEpoch 63/1000, Loss: 2.657518493277686\nEpoch 64/1000, Loss: 2.6536486936467036\nEpoch 65/1000, Loss: 2.6498240189892903\nEpoch 66/1000, Loss: 2.6460433581045697\nEpoch 67/1000, Loss: 2.6423057349664822\nEpoch 68/1000, Loss: 2.6386101001075337\nEpoch 69/1000, Loss: 2.6349555626511574\nEpoch 70/1000, Loss: 2.631341133798872\nEpoch 71/1000, Loss: 2.6277659237384796\nEpoch 72/1000, Loss: 2.624229071395738\nEpoch 73/1000, Loss: 2.620729730597564\nEpoch 74/1000, Loss: 2.6172671105180467\nEpoch 75/1000, Loss: 2.613840385207108\nEpoch 76/1000, Loss: 2.610448867082596\nEpoch 77/1000, Loss: 2.607091724872589\nEpoch 78/1000, Loss: 2.6037682922823087\nEpoch 79/1000, Loss: 2.60047788385834\nEpoch 80/1000, Loss: 2.597219799246107\nEpoch 81/1000, Loss: 2.593993403017521\nEpoch 82/1000, Loss: 2.5907980608088628\nEpoch 83/1000, Loss: 2.5876331499644687\nEpoch 84/1000, Loss: 2.5844980712447847\nEpoch 85/1000, Loss: 2.581392265856266\nEpoch 86/1000, Loss: 2.5783151164650917\nEpoch 87/1000, Loss: 2.575266103659357\nEpoch 88/1000, Loss: 2.572244644165039\nEpoch 89/1000, Loss: 2.569250328200204\nEpoch 90/1000, Loss: 2.5662825096930777\nEpoch 91/1000, Loss: 2.56334076076746\nEpoch 92/1000, Loss: 2.560424649289676\nEpoch 93/1000, Loss: 2.557533593050071\nEpoch 94/1000, Loss: 2.5546672056828226\nEpoch 95/1000, Loss: 2.5518250656979427\nEpoch 96/1000, Loss: 2.549006654747895\nEpoch 97/1000, Loss: 2.5462115949818065\nEpoch 98/1000, Loss: 2.543439503226961\nEpoch 99/1000, Loss: 2.540689893066883\nEpoch 100/1000, Loss: 2.5379624600921358\nEpoch 101/1000, Loss: 2.5352567487529347\nEpoch 102/1000, Loss: 2.532572413129466\nEpoch 103/1000, Loss: 2.5299091126237596\nEpoch 104/1000, Loss: 2.527266484286104\nEpoch 105/1000, Loss: 2.5246441481368884\nEpoch 106/1000, Loss: 2.522041730582714\nEpoch 107/1000, Loss: 2.5194590208785876\nEpoch 108/1000, Loss: 2.516895604985101\nEpoch 109/1000, Loss: 2.5143511593341827\nEpoch 110/1000, Loss: 2.511825429541724\nEpoch 111/1000, Loss: 2.5093180633016994\nEpoch 112/1000, Loss: 2.5068287466253554\nEpoch 113/1000, Loss: 2.504357264510223\nEpoch 114/1000, Loss: 2.501903291259493\nEpoch 115/1000, Loss: 2.499466610806329\nEpoch 116/1000, Loss: 2.4970468261412213\nEpoch 117/1000, Loss: 2.494643770158291\nEpoch 118/1000, Loss: 2.4922571906021664\nEpoch 119/1000, Loss: 2.4898868096726283\nEpoch 120/1000, Loss: 2.487532377243042\nEpoch 121/1000, Loss: 2.485193641058036\nEpoch 122/1000, Loss: 2.482870406338147\nEpoch 123/1000, Loss: 2.4805624218923703\nEpoch 124/1000, Loss: 2.4782694524952342\nEpoch 125/1000, Loss: 2.4759912863373756\nEpoch 126/1000, Loss: 2.4737276818071092\nEpoch 127/1000, Loss: 2.4714784984077727\nEpoch 128/1000, Loss: 2.4692434445023537\nEpoch 129/1000, Loss: 2.467022404074669\nEpoch 130/1000, Loss: 2.464815092938287\nEpoch 131/1000, Loss: 2.4626214003988673\nEpoch 132/1000, Loss: 2.4604410901665688\nEpoch 133/1000, Loss: 2.458274026002203\nEpoch 134/1000, Loss: 2.4561199620366096\nEpoch 135/1000, Loss: 2.4539787151983807\nEpoch 136/1000, Loss: 2.451850122639111\nEpoch 137/1000, Loss: 2.4497340651495114\nEpoch 138/1000, Loss: 2.447630342628275\nEpoch 139/1000, Loss: 2.4455387783902034\nEpoch 140/1000, Loss: 2.4434592745133807\nEpoch 141/1000, Loss: 2.4413915776780675\nEpoch 142/1000, Loss: 2.4393356101853505\nEpoch 143/1000, Loss: 2.4372911527752876\nEpoch 144/1000, Loss: 2.4352581117834364\nEpoch 145/1000, Loss: 2.4332363552280833\nEpoch 146/1000, Loss: 2.431225711745875\nEpoch 147/1000, Loss: 2.4292260333895683\nEpoch 148/1000, Loss: 2.4272372094648227\nEpoch 149/1000, Loss: 2.4252591154405048\nEpoch 150/1000, Loss: 2.4232915746314183\nEpoch 151/1000, Loss: 2.4213344688926424\nEpoch 152/1000, Loss: 2.4193877460701123\nEpoch 153/1000, Loss: 2.417451185839517\nEpoch 154/1000, Loss: 2.415524686021464\nEpoch 155/1000, Loss: 2.4136081667883054\nEpoch 156/1000, Loss: 2.4117015451192856\nEpoch 157/1000, Loss: 2.4098045911107744\nEpoch 158/1000, Loss: 2.4079172760248184\nEpoch 159/1000, Loss: 2.4060394689440727\nEpoch 160/1000, Loss: 2.404171086847782\nEpoch 161/1000, Loss: 2.4023119817887033\nEpoch 162/1000, Loss: 2.4004620388150215\nEpoch 163/1000, Loss: 2.3986212015151978\nEpoch 164/1000, Loss: 2.3967893655811037\nEpoch 165/1000, Loss: 2.3949664181896617\nEpoch 166/1000, Loss: 2.393152250775269\nEpoch 167/1000, Loss: 2.3913467494504794\nEpoch 168/1000, Loss: 2.389549896121025\nEpoch 169/1000, Loss: 2.387761503458023\nEpoch 170/1000, Loss: 2.3859815586890494\nEpoch 171/1000, Loss: 2.3842099915657724\nEpoch 172/1000, Loss: 2.3824466285961017\nEpoch 173/1000, Loss: 2.3806913920811246\nEpoch 174/1000, Loss: 2.378944279892104\nEpoch 175/1000, Loss: 2.37720514940364\nEpoch 176/1000, Loss: 2.375473951654775\nEpoch 177/1000, Loss: 2.3737505355051587\nEpoch 178/1000, Loss: 2.3720349531088556\nEpoch 179/1000, Loss: 2.370327011815139\nEpoch 180/1000, Loss: 2.368626662663051\nEpoch 181/1000, Loss: 2.3669338854295865\nEpoch 182/1000, Loss: 2.365248520459448\nEpoch 183/1000, Loss: 2.363570509212358\nEpoch 184/1000, Loss: 2.3618998442377364\nEpoch 185/1000, Loss: 2.3602364137768745\nEpoch 186/1000, Loss: 2.3585801827056065\nEpoch 187/1000, Loss: 2.3569310318146433\nEpoch 188/1000, Loss: 2.3552889046924457\nEpoch 189/1000, Loss: 2.353653771536691\nEpoch 190/1000, Loss: 2.3520255429404124\nEpoch 191/1000, Loss: 2.3504041688782826\nEpoch 192/1000, Loss: 2.3487895599433353\nEpoch 193/1000, Loss: 2.3471817022987773\nEpoch 194/1000, Loss: 2.3455804756709506\nEpoch 195/1000, Loss: 2.3439858885748044\nEpoch 196/1000, Loss: 2.3423978175435747\nEpoch 197/1000, Loss: 2.340816211487566\nEpoch 198/1000, Loss: 2.3392410555056165\nEpoch 199/1000, Loss: 2.3376722559332848\nEpoch 200/1000, Loss: 2.3361098234142577\nEpoch 201/1000, Loss: 2.3345536046794484\nEpoch 202/1000, Loss: 2.333003632724285\nEpoch 203/1000, Loss: 2.331459808562483\nEpoch 204/1000, Loss: 2.3299220811043466\nEpoch 205/1000, Loss: 2.3283903875521252\nEpoch 206/1000, Loss: 2.3268647279058183\nEpoch 207/1000, Loss: 2.3253449999860356\nEpoch 208/1000, Loss: 2.323831183569772\nEpoch 209/1000, Loss: 2.322323249919074\nEpoch 210/1000, Loss: 2.3208211021763936\nEpoch 211/1000, Loss: 2.3193247222474644\nEpoch 212/1000, Loss: 2.317834049463272\nEpoch 213/1000, Loss: 2.3163490561502322\nEpoch 214/1000, Loss: 2.3148696837680682\nEpoch 215/1000, Loss: 2.313395919544356\nEpoch 216/1000, Loss: 2.311927653849125\nEpoch 217/1000, Loss: 2.310464952673231\nEpoch 218/1000, Loss: 2.3090076691337993\nEpoch 219/1000, Loss: 2.307555797908987\nEpoch 220/1000, Loss: 2.306109269814832\nEpoch 221/1000, Loss: 2.3046680593064854\nEpoch 222/1000, Loss: 2.303232173834528\nEpoch 223/1000, Loss: 2.301801541021892\nEpoch 224/1000, Loss: 2.3003761087145125\nEpoch 225/1000, Loss: 2.2989558960710252\nEpoch 226/1000, Loss: 2.297540762594768\nEpoch 227/1000, Loss: 2.296130727444376\nEpoch 228/1000, Loss: 2.2947257501738414\nEpoch 229/1000, Loss: 2.2933258009808406\nEpoch 230/1000, Loss: 2.2919308436768397\nEpoch 231/1000, Loss: 2.290540882519313\nEpoch 232/1000, Loss: 2.2891557780759677\nEpoch 233/1000, Loss: 2.2877755803721294\nEpoch 234/1000, Loss: 2.2864002574767386\nEpoch 235/1000, Loss: 2.2850297253046716\nEpoch 236/1000, Loss: 2.2836639934352467\nEpoch 237/1000, Loss: 2.2823029373373305\nEpoch 238/1000, Loss: 2.280946647482259\nEpoch 239/1000, Loss: 2.2795950578791753\nEpoch 240/1000, Loss: 2.278248098279749\nEpoch 241/1000, Loss: 2.2769057165299142\nEpoch 242/1000, Loss: 2.275567893471037\nEpoch 243/1000, Loss: 2.2742347153169766\nEpoch 244/1000, Loss: 2.2729059851595332\nEpoch 245/1000, Loss: 2.271581833916051\nEpoch 246/1000, Loss: 2.2702620891588077\nEpoch 247/1000, Loss: 2.2689467881407057\nEpoch 248/1000, Loss: 2.2676358968019485\nEpoch 249/1000, Loss: 2.2663293800183704\nEpoch 250/1000, Loss: 2.265027219695704\nEpoch 251/1000, Loss: 2.2637293690017293\nEpoch 252/1000, Loss: 2.2624358353870258\nEpoch 253/1000, Loss: 2.2611465752124786\nEpoch 254/1000, Loss: 2.25986150013549\nEpoch 255/1000, Loss: 2.2585806686963354\nEpoch 256/1000, Loss: 2.2573040298053195\nEpoch 257/1000, Loss: 2.256031548338277\nEpoch 258/1000, Loss: 2.2547631998147284\nEpoch 259/1000, Loss: 2.2534989810415675\nEpoch 260/1000, Loss: 2.25223884837968\nEpoch 261/1000, Loss: 2.2509827305163657\nEpoch 262/1000, Loss: 2.249730736017227\nEpoch 263/1000, Loss: 2.248482631785529\nEpoch 264/1000, Loss: 2.2472385402236665\nEpoch 265/1000, Loss: 2.245998470910958\nEpoch 266/1000, Loss: 2.2447623206036433\nEpoch 267/1000, Loss: 2.2435300488557135\nEpoch 268/1000, Loss: 2.2423016907913342\nEpoch 269/1000, Loss: 2.2410771750978062\nEpoch 270/1000, Loss: 2.2398565390280316\nEpoch 271/1000, Loss: 2.238639713398048\nEpoch 272/1000, Loss: 2.23742667053427\nEpoch 273/1000, Loss: 2.2362174232091223\nEpoch 274/1000, Loss: 2.235011917139803\nEpoch 275/1000, Loss: 2.2338101427469934\nEpoch 276/1000, Loss: 2.2326120755502155\nEpoch 277/1000, Loss: 2.231417687875884\nEpoch 278/1000, Loss: 2.230226993560791\nEpoch 279/1000, Loss: 2.22903991064855\nEpoch 280/1000, Loss: 2.227856533867972\nEpoch 281/1000, Loss: 2.226676692920072\nEpoch 282/1000, Loss: 2.2255004761474475\nEpoch 283/1000, Loss: 2.224327806915556\nEpoch 284/1000, Loss: 2.223158678838185\nEpoch 285/1000, Loss: 2.2219930834003856\nEpoch 286/1000, Loss: 2.220831034438951\nEpoch 287/1000, Loss: 2.2196724265813828\nEpoch 288/1000, Loss: 2.2185172928231105\nEpoch 289/1000, Loss: 2.2173656299710274\nEpoch 290/1000, Loss: 2.2162173901285445\nEpoch 291/1000, Loss: 2.2150725988405093\nEpoch 292/1000, Loss: 2.2139311794723784\nEpoch 293/1000, Loss: 2.212793106479304\nEpoch 294/1000, Loss: 2.2116584703326225\nEpoch 295/1000, Loss: 2.210527108183929\nEpoch 296/1000, Loss: 2.2093990785734996\nEpoch 297/1000, Loss: 2.2082743644714355\nEpoch 298/1000, Loss: 2.2071529754570554\nEpoch 299/1000, Loss: 2.2060348242521286\nEpoch 300/1000, Loss: 2.204919919371605\nEpoch 301/1000, Loss: 2.2038083001971245\nEpoch 302/1000, Loss: 2.2026998102664948\nEpoch 303/1000, Loss: 2.201594594333853\nEpoch 304/1000, Loss: 2.2004925395761217\nEpoch 305/1000, Loss: 2.1993936875036786\nEpoch 306/1000, Loss: 2.198297957224505\nEpoch 307/1000, Loss: 2.1972053753478185\nEpoch 308/1000, Loss: 2.1961159322943007\nEpoch 309/1000, Loss: 2.1950295737811496\nEpoch 310/1000, Loss: 2.193946306194578\nEpoch 311/1000, Loss: 2.192866114633424\nEpoch 312/1000, Loss: 2.191788970359734\nEpoch 313/1000, Loss: 2.190714880824089\nEpoch 314/1000, Loss: 2.1896438215460097\nEpoch 315/1000, Loss: 2.188575748886381\nEpoch 316/1000, Loss: 2.1875107245785848\nEpoch 317/1000, Loss: 2.186448620898383\nEpoch 318/1000, Loss: 2.1853895666343823\nEpoch 319/1000, Loss: 2.184333431933607\nEpoch 320/1000, Loss: 2.1832801944443156\nEpoch 321/1000, Loss: 2.1822298978056227\nEpoch 322/1000, Loss: 2.1811825601117953\nEpoch 323/1000, Loss: 2.180138069604124\nEpoch 324/1000, Loss: 2.1790965273976326\nEpoch 325/1000, Loss: 2.178057810025556\nEpoch 326/1000, Loss: 2.1770219781569073\nEpoch 327/1000, Loss: 2.1759889838950977\nEpoch 328/1000, Loss: 2.1749588485274995\nEpoch 329/1000, Loss: 2.1739314879689897\nEpoch 330/1000, Loss: 2.1729069416012083\nEpoch 331/1000, Loss: 2.1718851923942566\nEpoch 332/1000, Loss: 2.170866233961923\nEpoch 333/1000, Loss: 2.169849982219083\nEpoch 334/1000, Loss: 2.168836542538234\nEpoch 335/1000, Loss: 2.1678258308342526\nEpoch 336/1000, Loss: 2.166817852428981\nEpoch 337/1000, Loss: 2.1658125732626234\nEpoch 338/1000, Loss: 2.164809986948967\nEpoch 339/1000, Loss: 2.163810078586851\nEpoch 340/1000, Loss: 2.16281287584986\nEpoch 341/1000, Loss: 2.161818299974714\nEpoch 342/1000, Loss: 2.16082635309015\nEpoch 343/1000, Loss: 2.1598370894789696\nEpoch 344/1000, Loss: 2.1588504825319563\nEpoch 345/1000, Loss: 2.157866457743304\nEpoch 346/1000, Loss: 2.1568850853613446\nEpoch 347/1000, Loss: 2.1559062568204745\nEpoch 348/1000, Loss: 2.1549299976655414\nEpoch 349/1000, Loss: 2.153956292995385\nEpoch 350/1000, Loss: 2.1529851758054326\nEpoch 351/1000, Loss: 2.1520166067140445\nEpoch 352/1000, Loss: 2.1510505846568515\nEpoch 353/1000, Loss: 2.150087039385523\nEpoch 354/1000, Loss: 2.1491260549851825\nEpoch 355/1000, Loss: 2.1481675186327527\nEpoch 356/1000, Loss: 2.1472115186708316\nEpoch 357/1000, Loss: 2.1462579497269223\nEpoch 358/1000, Loss: 2.1453069480402127\nEpoch 359/1000, Loss: 2.1443582975438664\nEpoch 360/1000, Loss: 2.1434121685368672\nEpoch 361/1000, Loss: 2.1424684301018715\nEpoch 362/1000, Loss: 2.1415271099124635\nEpoch 363/1000, Loss: 2.1405882548008646\nEpoch 364/1000, Loss: 2.139651778553213\nEpoch 365/1000, Loss: 2.138717637530395\nEpoch 366/1000, Loss: 2.137785947748593\nEpoch 367/1000, Loss: 2.136856638959476\nEpoch 368/1000, Loss: 2.135929678167616\nEpoch 369/1000, Loss: 2.13500504302127\nEpoch 370/1000, Loss: 2.1340828282492503\nEpoch 371/1000, Loss: 2.133162929543427\nEpoch 372/1000, Loss: 2.13224531603711\nEpoch 373/1000, Loss: 2.131330077137266\nEpoch 374/1000, Loss: 2.130417061703546\nEpoch 375/1000, Loss: 2.1295063410486494\nEpoch 376/1000, Loss: 2.128597958811692\nEpoch 377/1000, Loss: 2.127691816006388\nEpoch 378/1000, Loss: 2.126788003104074\nEpoch 379/1000, Loss: 2.1258863849299297\nEpoch 380/1000, Loss: 2.124987045569079\nEpoch 381/1000, Loss: 2.1240899254168784\nEpoch 382/1000, Loss: 2.1231950159583772\nEpoch 383/1000, Loss: 2.122302404471806\nEpoch 384/1000, Loss: 2.121411925980023\nEpoch 385/1000, Loss: 2.1205237358808517\nEpoch 386/1000, Loss: 2.1196377405098508\nEpoch 387/1000, Loss: 2.1187538940991675\nEpoch 388/1000, Loss: 2.1178721828120097\nEpoch 389/1000, Loss: 2.116992727986404\nEpoch 390/1000, Loss: 2.1161153678383147\nEpoch 391/1000, Loss: 2.1152401428137506\nEpoch 392/1000, Loss: 2.114367090165615\nEpoch 393/1000, Loss: 2.1134961726410046\nEpoch 394/1000, Loss: 2.112627340214593\nEpoch 395/1000, Loss: 2.1117606280105456\nEpoch 396/1000, Loss: 2.1108960615737096\nEpoch 397/1000, Loss: 2.110033554690225\nEpoch 398/1000, Loss: 2.109173118003777\nEpoch 399/1000, Loss: 2.1083148355994905\nEpoch 400/1000, Loss: 2.1074585797531262\nEpoch 401/1000, Loss: 2.1066044164555415\nEpoch 402/1000, Loss: 2.1057523680584773\nEpoch 403/1000, Loss: 2.104902278099741\nEpoch 404/1000, Loss: 2.104054284947259\nEpoch 405/1000, Loss: 2.1032083045159067\nEpoch 406/1000, Loss: 2.1023643080677306\nEpoch 407/1000, Loss: 2.1015224031039645\nEpoch 408/1000, Loss: 2.100682464029108\nEpoch 409/1000, Loss: 2.0998445195811137\nEpoch 410/1000, Loss: 2.0990085942404613\nEpoch 411/1000, Loss: 2.09817468055657\nEpoch 412/1000, Loss: 2.097342752984592\nEpoch 413/1000, Loss: 2.0965127625635693\nEpoch 414/1000, Loss: 2.0956847348383496\nEpoch 415/1000, Loss: 2.0948586879032\nEpoch 416/1000, Loss: 2.094034569604056\nEpoch 417/1000, Loss: 2.093212461897305\nEpoch 418/1000, Loss: 2.092392213642597\nEpoch 419/1000, Loss: 2.0915739131825313\nEpoch 420/1000, Loss: 2.0907575115561485\nEpoch 421/1000, Loss: 2.089943059853145\nEpoch 422/1000, Loss: 2.089130529335567\nEpoch 423/1000, Loss: 2.088319832725184\nEpoch 424/1000, Loss: 2.087511089231287\nEpoch 425/1000, Loss: 2.0867042051894322\nEpoch 426/1000, Loss: 2.085899218916893\nEpoch 427/1000, Loss: 2.085096076130867\nEpoch 428/1000, Loss: 2.0842948321785246\nEpoch 429/1000, Loss: 2.0834954338414327\nEpoch 430/1000, Loss: 2.082697864089693\nEpoch 431/1000, Loss: 2.081902145275048\nEpoch 432/1000, Loss: 2.081108312521662\nEpoch 433/1000, Loss: 2.0803162764225687\nEpoch 434/1000, Loss: 2.079526111483574\nEpoch 435/1000, Loss: 2.0787377368126596\nEpoch 436/1000, Loss: 2.0779512226581573\nEpoch 437/1000, Loss: 2.077166494514261\nEpoch 438/1000, Loss: 2.076383553445339\nEpoch 439/1000, Loss: 2.07560237709965\nEpoch 440/1000, Loss: 2.07482298463583\nEpoch 441/1000, Loss: 2.0740453898906708\nEpoch 442/1000, Loss: 2.0732695619974817\nEpoch 443/1000, Loss: 2.072495512664318\nEpoch 444/1000, Loss: 2.0717231801577976\nEpoch 445/1000, Loss: 2.0709526549492563\nEpoch 446/1000, Loss: 2.0701838689191\nEpoch 447/1000, Loss: 2.069416814616748\nEpoch 448/1000, Loss: 2.068651495235307\nEpoch 449/1000, Loss: 2.0678879065173015\nEpoch 450/1000, Loss: 2.0671260537845746\nEpoch 451/1000, Loss: 2.0663658593382155\nEpoch 452/1000, Loss: 2.065607484962259\nEpoch 453/1000, Loss: 2.0648508114474162\nEpoch 454/1000, Loss: 2.064095774931567\nEpoch 455/1000, Loss: 2.0633424456630434\nEpoch 456/1000, Loss: 2.0625908098050525\nEpoch 457/1000, Loss: 2.0618408301046918\nEpoch 458/1000, Loss: 2.061092545943601\nEpoch 459/1000, Loss: 2.0603459860597337\nEpoch 460/1000, Loss: 2.059601044016225\nEpoch 461/1000, Loss: 2.0588577730315074\nEpoch 462/1000, Loss: 2.0581160954066684\nEpoch 463/1000, Loss: 2.0573761111923625\nEpoch 464/1000, Loss: 2.0566378459334373\nEpoch 465/1000, Loss: 2.0559011633907045\nEpoch 466/1000, Loss: 2.055166119975703\nEpoch 467/1000, Loss: 2.054432708237852\nEpoch 468/1000, Loss: 2.053700895181724\nEpoch 469/1000, Loss: 2.0529706946441104\nEpoch 470/1000, Loss: 2.0522421364273344\nEpoch 471/1000, Loss: 2.051515156669276\nEpoch 472/1000, Loss: 2.0507897809147835\nEpoch 473/1000, Loss: 2.0500660219362805\nEpoch 474/1000, Loss: 2.0493438009704863\nEpoch 475/1000, Loss: 2.0486231967806816\nEpoch 476/1000, Loss: 2.047904151890959\nEpoch 477/1000, Loss: 2.0471867056829587\nEpoch 478/1000, Loss: 2.0464707357542857\nEpoch 479/1000, Loss: 2.045756472008569\nEpoch 480/1000, Loss: 2.0450437058295523\nEpoch 481/1000, Loss: 2.0443325330104147\nEpoch 482/1000, Loss: 2.0436228087970187\nEpoch 483/1000, Loss: 2.0429147151964053\nEpoch 484/1000, Loss: 2.042208124484335\nEpoch 485/1000, Loss: 2.0415030920079777\nEpoch 486/1000, Loss: 2.0407995922224864\nEpoch 487/1000, Loss: 2.0400975814887454\nEpoch 488/1000, Loss: 2.0393970949309215\nEpoch 489/1000, Loss: 2.038698115519115\nEpoch 490/1000, Loss: 2.0380006496395384\nEpoch 491/1000, Loss: 2.0373046909059798\nEpoch 492/1000, Loss: 2.036610239318439\nEpoch 493/1000, Loss: 2.035917279975755\nEpoch 494/1000, Loss: 2.0352257766893933\nEpoch 495/1000, Loss: 2.034535856119224\nEpoch 496/1000, Loss: 2.03384735754558\nEpoch 497/1000, Loss: 2.0331603458949496\nEpoch 498/1000, Loss: 2.0324747881719043\nEpoch 499/1000, Loss: 2.031790755689144\nEpoch 500/1000, Loss: 2.031108118593693\nEpoch 501/1000, Loss: 2.030426984386785\nEpoch 502/1000, Loss: 2.0297473168798854\nEpoch 503/1000, Loss: 2.0290690841419354\nEpoch 504/1000, Loss: 2.028392263821193\nEpoch 505/1000, Loss: 2.027716896363667\nEpoch 506/1000, Loss: 2.0270429689969336\nEpoch 507/1000, Loss: 2.0263704689485684\nEpoch 508/1000, Loss: 2.0256994047335217\nEpoch 509/1000, Loss: 2.025029733777046\nEpoch 510/1000, Loss: 2.0243615167481557\nEpoch 511/1000, Loss: 2.0236947408744266\nEpoch 512/1000, Loss: 2.023029312491417\nEpoch 513/1000, Loss: 2.0223653284566745\nEpoch 514/1000, Loss: 2.0217027195862363\nEpoch 515/1000, Loss: 2.0210414731076787\nEpoch 516/1000, Loss: 2.0203816848141805\nEpoch 517/1000, Loss: 2.0197232759424617\nEpoch 518/1000, Loss: 2.0190662624580518\nEpoch 519/1000, Loss: 2.0184105911425183\nEpoch 520/1000, Loss: 2.017756319471768\nEpoch 521/1000, Loss: 2.017103516629764\nEpoch 522/1000, Loss: 2.0164519771933556\nEpoch 523/1000, Loss: 2.015801760767187\nEpoch 524/1000, Loss: 2.015152968466282\nEpoch 525/1000, Loss: 2.0145055215273584\nEpoch 526/1000, Loss: 2.013859431658472\nEpoch 527/1000, Loss: 2.0132146573492458\nEpoch 528/1000, Loss: 2.0125712464962686\nEpoch 529/1000, Loss: 2.0119292235800197\nEpoch 530/1000, Loss: 2.01128852473838\nEpoch 531/1000, Loss: 2.0106491340058192\nEpoch 532/1000, Loss: 2.010011039674282\nEpoch 533/1000, Loss: 2.0093743492450034\nEpoch 534/1000, Loss: 2.0087389275431633\nEpoch 535/1000, Loss: 2.0081048575895175\nEpoch 536/1000, Loss: 2.0074721159679547\nEpoch 537/1000, Loss: 2.006840708000319\nEpoch 538/1000, Loss: 2.0062105325715884\nEpoch 539/1000, Loss: 2.0055817269853184\nEpoch 540/1000, Loss: 2.004954177354063\nEpoch 541/1000, Loss: 2.0043280018227443\nEpoch 542/1000, Loss: 2.003703031156744\nEpoch 543/1000, Loss: 2.0030794388481548\nEpoch 544/1000, Loss: 2.002457132296903\nEpoch 545/1000, Loss: 2.001836055091449\nEpoch 546/1000, Loss: 2.001216239162854\nEpoch 547/1000, Loss: 2.0005977089915956\nEpoch 548/1000, Loss: 1.9999804816075735\nEpoch 549/1000, Loss: 1.9993644250290734\nEpoch 550/1000, Loss: 1.9987496840102332\nEpoch 551/1000, Loss: 1.998136195753302\nEpoch 552/1000, Loss: 1.9975239858031273\nEpoch 553/1000, Loss: 1.9969130530953407\nEpoch 554/1000, Loss: 1.996303373149463\nEpoch 555/1000, Loss: 1.9956949321287019\nEpoch 556/1000, Loss: 1.99508771193879\nEpoch 557/1000, Loss: 1.9944817179015704\nEpoch 558/1000, Loss: 1.993876973433154\nEpoch 559/1000, Loss: 1.9932734870484896\nEpoch 560/1000, Loss: 1.992671254490103\nEpoch 561/1000, Loss: 1.9920702746936254\nEpoch 562/1000, Loss: 1.991470456123352\nEpoch 563/1000, Loss: 1.9908719414046832\nEpoch 564/1000, Loss: 1.9902745708823204\nEpoch 565/1000, Loss: 1.9896784541862351\nEpoch 566/1000, Loss: 1.9890835231968336\nEpoch 567/1000, Loss: 1.988489871578557\nEpoch 568/1000, Loss: 1.9878974195037569\nEpoch 569/1000, Loss: 1.9873060903378896\nEpoch 570/1000, Loss: 1.9867160650236266\nEpoch 571/1000, Loss: 1.9861271988068308\nEpoch 572/1000, Loss: 1.9855394991380828\nEpoch 573/1000, Loss: 1.984953063939299\nEpoch 574/1000, Loss: 1.9843678016747748\nEpoch 575/1000, Loss: 1.983783666576658\nEpoch 576/1000, Loss: 1.9832007576312338\nEpoch 577/1000, Loss: 1.9826190322637558\nEpoch 578/1000, Loss: 1.982038485152381\nEpoch 579/1000, Loss: 1.981459106717791\nEpoch 580/1000, Loss: 1.9808809139898844\nEpoch 581/1000, Loss: 1.9803038803594453\nEpoch 582/1000, Loss: 1.9797280537230628\nEpoch 583/1000, Loss: 1.979153346802507\nEpoch 584/1000, Loss: 1.978579809623105\nEpoch 585/1000, Loss: 1.9780074836952346\nEpoch 586/1000, Loss: 1.9774362572601862\nEpoch 587/1000, Loss: 1.9768662016306604\nEpoch 588/1000, Loss: 1.9762973146779197\nEpoch 589/1000, Loss: 1.9757295516984803\nEpoch 590/1000, Loss: 1.9751628818256515\nEpoch 591/1000, Loss: 1.9745974019169807\nEpoch 592/1000, Loss: 1.974033032144819\nEpoch 593/1000, Loss: 1.9734698746885573\nEpoch 594/1000, Loss: 1.9729077911802702\nEpoch 595/1000, Loss: 1.9723468337740218\nEpoch 596/1000, Loss: 1.9717870141778673\nEpoch 597/1000, Loss: 1.9712283515504427\nEpoch 598/1000, Loss: 1.970670806510108\nEpoch 599/1000, Loss: 1.9701143795890468\nEpoch 600/1000, Loss: 1.9695590500320708\nEpoch 601/1000, Loss: 1.9690048721219813\nEpoch 602/1000, Loss: 1.968451769224235\nEpoch 603/1000, Loss: 1.9678998131837164\nEpoch 604/1000, Loss: 1.9673489651509695\nEpoch 605/1000, Loss: 1.9667992080960954\nEpoch 606/1000, Loss: 1.9662505664995737\nEpoch 607/1000, Loss: 1.9657030286533492\nEpoch 608/1000, Loss: 1.965156574334417\nEpoch 609/1000, Loss: 1.9646112535681044\nEpoch 610/1000, Loss: 1.9640670200543744\nEpoch 611/1000, Loss: 1.9635238679391998\nEpoch 612/1000, Loss: 1.9629818073340826\nEpoch 613/1000, Loss: 1.962440809501069\nEpoch 614/1000, Loss: 1.9619008717792374\nEpoch 615/1000, Loss: 1.961362092622689\nEpoch 616/1000, Loss: 1.960824340581894\nEpoch 617/1000, Loss: 1.9602876779224192\nEpoch 618/1000, Loss: 1.9597521243350846\nEpoch 619/1000, Loss: 1.9592176739658629\nEpoch 620/1000, Loss: 1.958684225167547\nEpoch 621/1000, Loss: 1.9581518833126341\nEpoch 622/1000, Loss: 1.9576206356287003\nEpoch 623/1000, Loss: 1.9570904523134232\nEpoch 624/1000, Loss: 1.9565612939851624\nEpoch 625/1000, Loss: 1.9560332287635123\nEpoch 626/1000, Loss: 1.9555062071553297\nEpoch 627/1000, Loss: 1.9549802488514356\nEpoch 628/1000, Loss: 1.9544553575771195\nEpoch 629/1000, Loss: 1.9539314923541886\nEpoch 630/1000, Loss: 1.953408689371177\nEpoch 631/1000, Loss: 1.9528869645936149\nEpoch 632/1000, Loss: 1.9523663084421838\nEpoch 633/1000, Loss: 1.9518466969685895\nEpoch 634/1000, Loss: 1.9513280210750443\nEpoch 635/1000, Loss: 1.950810440416847\nEpoch 636/1000, Loss: 1.9502939422215735\nEpoch 637/1000, Loss: 1.949778471674238\nEpoch 638/1000, Loss: 1.949264063366822\nEpoch 639/1000, Loss: 1.948750609798091\nEpoch 640/1000, Loss: 1.9482382418853896\nEpoch 641/1000, Loss: 1.9477269000240736\nEpoch 642/1000, Loss: 1.9472165703773499\nEpoch 643/1000, Loss: 1.9467072859406471\nEpoch 644/1000, Loss: 1.9461989120713301\nEpoch 645/1000, Loss: 1.9456916733511858\nEpoch 646/1000, Loss: 1.9451854165111269\nEpoch 647/1000, Loss: 1.9446801277143615\nEpoch 648/1000, Loss: 1.944175903818437\nEpoch 649/1000, Loss: 1.943672670849732\nEpoch 650/1000, Loss: 1.9431704474346978\nEpoch 651/1000, Loss: 1.9426692223974638\nEpoch 652/1000, Loss: 1.9421690276690893\nEpoch 653/1000, Loss: 1.9416698281254088\nEpoch 654/1000, Loss: 1.9411717179630483\nEpoch 655/1000, Loss: 1.940674539655447\nEpoch 656/1000, Loss: 1.940178376755544\nEpoch 657/1000, Loss: 1.9396832500185286\nEpoch 658/1000, Loss: 1.9391889960638113\nEpoch 659/1000, Loss: 1.9386958458593913\nEpoch 660/1000, Loss: 1.9382037137235915\nEpoch 661/1000, Loss: 1.9377125012023109\nEpoch 662/1000, Loss: 1.9372223259082861\nEpoch 663/1000, Loss: 1.9367331096104212\nEpoch 664/1000, Loss: 1.9362448395362921\nEpoch 665/1000, Loss: 1.9357576136078154\nEpoch 666/1000, Loss: 1.935271359447922\nEpoch 667/1000, Loss: 1.9347860238381795\nEpoch 668/1000, Loss: 1.9343016828809465\nEpoch 669/1000, Loss: 1.9338183881981033\nEpoch 670/1000, Loss: 1.9333360844424792\nEpoch 671/1000, Loss: 1.9328547354255403\nEpoch 672/1000, Loss: 1.9323743704174245\nEpoch 673/1000, Loss: 1.9318948835134506\nEpoch 674/1000, Loss: 1.9314163944550924\nEpoch 675/1000, Loss: 1.9309389303837503\nEpoch 676/1000, Loss: 1.9304624604327338\nEpoch 677/1000, Loss: 1.929986881358283\nEpoch 678/1000, Loss: 1.9295122820351804\nEpoch 679/1000, Loss: 1.9290386736392975\nEpoch 680/1000, Loss: 1.9285659566521645\nEpoch 681/1000, Loss: 1.928094214626721\nEpoch 682/1000, Loss: 1.9276234869446074\nEpoch 683/1000, Loss: 1.9271536490746908\nEpoch 684/1000, Loss: 1.92668481277568\nEpoch 685/1000, Loss: 1.9262169493096215\nEpoch 686/1000, Loss: 1.9257499809776033\nEpoch 687/1000, Loss: 1.9252840115555696\nEpoch 688/1000, Loss: 1.9248189383319445\nEpoch 689/1000, Loss: 1.924354872001069\nEpoch 690/1000, Loss: 1.923891696546759\nEpoch 691/1000, Loss: 1.9234294870070048\nEpoch 692/1000, Loss: 1.9229682396565164\nEpoch 693/1000, Loss: 1.9225079007446766\nEpoch 694/1000, Loss: 1.9220485463738441\nEpoch 695/1000, Loss: 1.9215900488197803\nEpoch 696/1000, Loss: 1.9211325427251202\nEpoch 697/1000, Loss: 1.9206759525196893\nEpoch 698/1000, Loss: 1.9202202835253306\nEpoch 699/1000, Loss: 1.9197654899741923\nEpoch 700/1000, Loss: 1.91931170171925\nEpoch 701/1000, Loss: 1.9188588070017951\nEpoch 702/1000, Loss: 1.9184068499931268\nEpoch 703/1000, Loss: 1.9179558392081941\nEpoch 704/1000, Loss: 1.917505721960749\nEpoch 705/1000, Loss: 1.9170565195381641\nEpoch 706/1000, Loss: 1.9166082218289375\nEpoch 707/1000, Loss: 1.9161608714078153\nEpoch 708/1000, Loss: 1.91571444272995\nEpoch 709/1000, Loss: 1.9152689299413137\nEpoch 710/1000, Loss: 1.9148242878062385\nEpoch 711/1000, Loss: 1.9143806541604655\nEpoch 712/1000, Loss: 1.913937873606171\nEpoch 713/1000, Loss: 1.9134960296962942\nEpoch 714/1000, Loss: 1.9130550069468362\nEpoch 715/1000, Loss: 1.9126150006694453\nEpoch 716/1000, Loss: 1.9121758921870164\nEpoch 717/1000, Loss: 1.9117376878857613\nEpoch 718/1000, Loss: 1.911300356898989\nEpoch 719/1000, Loss: 1.9108639066772801\nEpoch 720/1000, Loss: 1.9104283952287264\nEpoch 721/1000, Loss: 1.9099937970084804\nEpoch 722/1000, Loss: 1.9095600694417953\nEpoch 723/1000, Loss: 1.9091272779873438\nEpoch 724/1000, Loss: 1.908695322062288\nEpoch 725/1000, Loss: 1.9082643677081381\nEpoch 726/1000, Loss: 1.9078342361109597\nEpoch 727/1000, Loss: 1.907404980489186\nEpoch 728/1000, Loss: 1.9069765912634986\nEpoch 729/1000, Loss: 1.906549081738506\nEpoch 730/1000, Loss: 1.906122446592365\nEpoch 731/1000, Loss: 1.9056968092918396\nEpoch 732/1000, Loss: 1.9052720053919725\nEpoch 733/1000, Loss: 1.9048480694847447\nEpoch 734/1000, Loss: 1.9044250366943223\nEpoch 735/1000, Loss: 1.9040028953126498\nEpoch 736/1000, Loss: 1.903581663966179\nEpoch 737/1000, Loss: 1.903161249522652\nEpoch 738/1000, Loss: 1.902741715312004\nEpoch 739/1000, Loss: 1.9023230139698302\nEpoch 740/1000, Loss: 1.901905280138765\nEpoch 741/1000, Loss: 1.9014883626784598\nEpoch 742/1000, Loss: 1.9010723398200102\nEpoch 743/1000, Loss: 1.900657200387546\nEpoch 744/1000, Loss: 1.900242883179869\nEpoch 745/1000, Loss: 1.8998294903763704\nEpoch 746/1000, Loss: 1.8994169346988201\nEpoch 747/1000, Loss: 1.8990052055035318\nEpoch 748/1000, Loss: 1.8985943629273347\nEpoch 749/1000, Loss: 1.8981844085667814\nEpoch 750/1000, Loss: 1.8977752306631632\nEpoch 751/1000, Loss: 1.897366948957954\nEpoch 752/1000, Loss: 1.8969594889453478\nEpoch 753/1000, Loss: 1.8965529671737127\nEpoch 754/1000, Loss: 1.896147302218846\nEpoch 755/1000, Loss: 1.8957424366048403\nEpoch 756/1000, Loss: 1.8953384682536125\nEpoch 757/1000, Loss: 1.8949353923755032\nEpoch 758/1000, Loss: 1.8945331115807806\nEpoch 759/1000, Loss: 1.894131706229278\nEpoch 760/1000, Loss: 1.8937311289565903\nEpoch 761/1000, Loss: 1.8933314404317312\nEpoch 762/1000, Loss: 1.8929326034017973\nEpoch 763/1000, Loss: 1.8925346162702357\nEpoch 764/1000, Loss: 1.8921374502990926\nEpoch 765/1000, Loss: 1.8917411438056402\nEpoch 766/1000, Loss: 1.8913456499576569\nEpoch 767/1000, Loss: 1.8909510225057602\nEpoch 768/1000, Loss: 1.8905571842832225\nEpoch 769/1000, Loss: 1.8901642188429832\nEpoch 770/1000, Loss: 1.8897720538079739\nEpoch 771/1000, Loss: 1.8893807913575853\nEpoch 772/1000, Loss: 1.8889903176043714\nEpoch 773/1000, Loss: 1.888600651706968\nEpoch 774/1000, Loss: 1.8882118128240108\nEpoch 775/1000, Loss: 1.8878238451267992\nEpoch 776/1000, Loss: 1.8874366331313337\nEpoch 777/1000, Loss: 1.8870503364929132\nEpoch 778/1000, Loss: 1.8866648604827267\nEpoch 779/1000, Loss: 1.8862801933927196\nEpoch 780/1000, Loss: 1.8858964049390383\nEpoch 781/1000, Loss: 1.885513361543417\nEpoch 782/1000, Loss: 1.8851311169564724\nEpoch 783/1000, Loss: 1.8847497824047292\nEpoch 784/1000, Loss: 1.8843692067478384\nEpoch 785/1000, Loss: 1.8839894458651543\nEpoch 786/1000, Loss: 1.8836105880992753\nEpoch 787/1000, Loss: 1.883232475391456\nEpoch 788/1000, Loss: 1.8828552099210876\nEpoch 789/1000, Loss: 1.8824786698179585\nEpoch 790/1000, Loss: 1.8821030200592108\nEpoch 791/1000, Loss: 1.8817281861390387\nEpoch 792/1000, Loss: 1.8813541611390454\nEpoch 793/1000, Loss: 1.8809808987591947\nEpoch 794/1000, Loss: 1.8806084410420485\nEpoch 795/1000, Loss: 1.8802368018243993\nEpoch 796/1000, Loss: 1.8798659640763487\nEpoch 797/1000, Loss: 1.8794959549392973\nEpoch 798/1000, Loss: 1.8791267180017062\nEpoch 799/1000, Loss: 1.8787582921130317\nEpoch 800/1000, Loss: 1.8783906527927943\nEpoch 801/1000, Loss: 1.878023830375501\nEpoch 802/1000, Loss: 1.8776578669037138\nEpoch 803/1000, Loss: 1.877292635185378\nEpoch 804/1000, Loss: 1.8769282262240137\nEpoch 805/1000, Loss: 1.8765646182000637\nEpoch 806/1000, Loss: 1.876201833997454\nEpoch 807/1000, Loss: 1.8758397639862128\nEpoch 808/1000, Loss: 1.8754785428089755\nEpoch 809/1000, Loss: 1.8751180634966917\nEpoch 810/1000, Loss: 1.8747584143919604\nEpoch 811/1000, Loss: 1.874399563031537\nEpoch 812/1000, Loss: 1.874041494514261\nEpoch 813/1000, Loss: 1.8736841822309154\nEpoch 814/1000, Loss: 1.873327659709113\nEpoch 815/1000, Loss: 1.872971938656909\nEpoch 816/1000, Loss: 1.8726170063018799\nEpoch 817/1000, Loss: 1.872262857322182\nEpoch 818/1000, Loss: 1.871909418276378\nEpoch 819/1000, Loss: 1.8715568371117115\nEpoch 820/1000, Loss: 1.871204956301621\nEpoch 821/1000, Loss: 1.8708538982485021\nEpoch 822/1000, Loss: 1.8705035378890378\nEpoch 823/1000, Loss: 1.870154051908425\nEpoch 824/1000, Loss: 1.8698052604283606\nEpoch 825/1000, Loss: 1.8694572799972125\nEpoch 826/1000, Loss: 1.8691100073712212\nEpoch 827/1000, Loss: 1.8687635739999158\nEpoch 828/1000, Loss: 1.8684179005878312\nEpoch 829/1000, Loss: 1.8680729174188204\nEpoch 830/1000, Loss: 1.8677287570067815\nEpoch 831/1000, Loss: 1.8673853538930416\nEpoch 832/1000, Loss: 1.867042738944292\nEpoch 833/1000, Loss: 1.8667008908731597\nEpoch 834/1000, Loss: 1.8663597702980042\nEpoch 835/1000, Loss: 1.8660194341625487\nEpoch 836/1000, Loss: 1.865679822329964\nEpoch 837/1000, Loss: 1.8653409661991256\nEpoch 838/1000, Loss: 1.8650029126022543\nEpoch 839/1000, Loss: 1.8646655907588345\nEpoch 840/1000, Loss: 1.864329000136682\nEpoch 841/1000, Loss: 1.863993122109345\nEpoch 842/1000, Loss: 1.8636580051055975\nEpoch 843/1000, Loss: 1.8633236927645547\nEpoch 844/1000, Loss: 1.8629900876964842\nEpoch 845/1000, Loss: 1.8626572553600584\nEpoch 846/1000, Loss: 1.8623251712747984\nEpoch 847/1000, Loss: 1.8619938205395425\nEpoch 848/1000, Loss: 1.8616631999611855\nEpoch 849/1000, Loss: 1.8613333361489433\nEpoch 850/1000, Loss: 1.8610041875924384\nEpoch 851/1000, Loss: 1.8606757840939931\nEpoch 852/1000, Loss: 1.8603481017053127\nEpoch 853/1000, Loss: 1.8600211899195398\nEpoch 854/1000, Loss: 1.8596950226596423\nEpoch 855/1000, Loss: 1.8593695935394083\nEpoch 856/1000, Loss: 1.8590448876576764\nEpoch 857/1000, Loss: 1.8587209124650275\nEpoch 858/1000, Loss: 1.8583976860557283\nEpoch 859/1000, Loss: 1.8580751360527106\nEpoch 860/1000, Loss: 1.8577533247215408\nEpoch 861/1000, Loss: 1.857432346791029\nEpoch 862/1000, Loss: 1.857111929250615\nEpoch 863/1000, Loss: 1.8567922956177167\nEpoch 864/1000, Loss: 1.8564734192831176\nEpoch 865/1000, Loss: 1.8561552198869842\nEpoch 866/1000, Loss: 1.8558378091880254\nEpoch 867/1000, Loss: 1.8555210663803987\nEpoch 868/1000, Loss: 1.855205045214721\nEpoch 869/1000, Loss: 1.8548897957163197\nEpoch 870/1000, Loss: 1.8545752220920153\nEpoch 871/1000, Loss: 1.8542613610625267\nEpoch 872/1000, Loss: 1.8539482317864895\nEpoch 873/1000, Loss: 1.8536357709339686\nEpoch 874/1000, Loss: 1.8533240354486875\nEpoch 875/1000, Loss: 1.8530130263950144\nEpoch 876/1000, Loss: 1.852702725146498\nEpoch 877/1000, Loss: 1.8523930683732033\nEpoch 878/1000, Loss: 1.8520841992327146\nEpoch 879/1000, Loss: 1.851775927203042\nEpoch 880/1000, Loss: 1.8514684098107475\nEpoch 881/1000, Loss: 1.8511615970305033\nEpoch 882/1000, Loss: 1.8508554872657572\nEpoch 883/1000, Loss: 1.850550034216472\nEpoch 884/1000, Loss: 1.8502453198390347\nEpoch 885/1000, Loss: 1.8499412781425886\nEpoch 886/1000, Loss: 1.8496379368007183\nEpoch 887/1000, Loss: 1.849335253238678\nEpoch 888/1000, Loss: 1.8490333440048354\nEpoch 889/1000, Loss: 1.848732138318675\nEpoch 890/1000, Loss: 1.8484315377260958\nEpoch 891/1000, Loss: 1.848131642809936\nEpoch 892/1000, Loss: 1.8478324051414217\nEpoch 893/1000, Loss: 1.8475338625056403\nEpoch 894/1000, Loss: 1.8472360335290432\nEpoch 895/1000, Loss: 1.8469389719622475\nEpoch 896/1000, Loss: 1.8466424739786558\nEpoch 897/1000, Loss: 1.8463466800749302\nEpoch 898/1000, Loss: 1.8460515620453017\nEpoch 899/1000, Loss: 1.8457571587392263\nEpoch 900/1000, Loss: 1.8454634031014783\nEpoch 901/1000, Loss: 1.845170311097588\nEpoch 902/1000, Loss: 1.8448779162551676\nEpoch 903/1000, Loss: 1.8445861648236002\nEpoch 904/1000, Loss: 1.8442952084754194\nEpoch 905/1000, Loss: 1.8440048220966543\nEpoch 906/1000, Loss: 1.8437151589563914\nEpoch 907/1000, Loss: 1.8434261147465025\nEpoch 908/1000, Loss: 1.8431378028222494\nEpoch 909/1000, Loss: 1.8428500869444437\nEpoch 910/1000, Loss: 1.8425630070269108\nEpoch 911/1000, Loss: 1.8422765960650784\nEpoch 912/1000, Loss: 1.8419908774750573\nEpoch 913/1000, Loss: 1.8417058432740825\nEpoch 914/1000, Loss: 1.841421454612698\nEpoch 915/1000, Loss: 1.8411377221345901\nEpoch 916/1000, Loss: 1.8408546506294183\nEpoch 917/1000, Loss: 1.8405722486121314\nEpoch 918/1000, Loss: 1.8402905107608862\nEpoch 919/1000, Loss: 1.8400094200457846\nEpoch 920/1000, Loss: 1.8397289881748813\nEpoch 921/1000, Loss: 1.8394491991826467\nEpoch 922/1000, Loss: 1.8391700674380576\nEpoch 923/1000, Loss: 1.8388915902801923\nEpoch 924/1000, Loss: 1.838613750146968\nEpoch 925/1000, Loss: 1.8383364927555834\nEpoch 926/1000, Loss: 1.8380599160279547\nEpoch 927/1000, Loss: 1.837783966745649\nEpoch 928/1000, Loss: 1.8375086284109525\nEpoch 929/1000, Loss: 1.8372339547744818\nEpoch 930/1000, Loss: 1.8369599639305048\nEpoch 931/1000, Loss: 1.8366865632789475\nEpoch 932/1000, Loss: 1.8364138374371188\nEpoch 933/1000, Loss: 1.8361417725682259\nEpoch 934/1000, Loss: 1.8358703197113104\nEpoch 935/1000, Loss: 1.8355994895100594\nEpoch 936/1000, Loss: 1.8353292697242327\nEpoch 937/1000, Loss: 1.8350596481135912\nEpoch 938/1000, Loss: 1.834790667252881\nEpoch 939/1000, Loss: 1.834522331399577\nEpoch 940/1000, Loss: 1.8342546155410153\nEpoch 941/1000, Loss: 1.8339875606553895\nEpoch 942/1000, Loss: 1.8337211055415017\nEpoch 943/1000, Loss: 1.8334552837269646\nEpoch 944/1000, Loss: 1.8331901069198335\nEpoch 945/1000, Loss: 1.8329255293522562\nEpoch 946/1000, Loss: 1.8326615472989423\nEpoch 947/1000, Loss: 1.8323982177036149\nEpoch 948/1000, Loss: 1.8321355171501637\nEpoch 949/1000, Loss: 1.8318734089178699\nEpoch 950/1000, Loss: 1.8316119494182723\nEpoch 951/1000, Loss: 1.8313510396650858\nEpoch 952/1000, Loss: 1.8310907876917295\nEpoch 953/1000, Loss: 1.8308311477303505\nEpoch 954/1000, Loss: 1.8305720346314567\nEpoch 955/1000, Loss: 1.8303136357239314\nEpoch 956/1000, Loss: 1.8300557716616563\nEpoch 957/1000, Loss: 1.829798526529755\nEpoch 958/1000, Loss: 1.8295418854270662\nEpoch 959/1000, Loss: 1.829285800989185\nEpoch 960/1000, Loss: 1.829030356769051\nEpoch 961/1000, Loss: 1.8287755490413733\nEpoch 962/1000, Loss: 1.8285213144762176\nEpoch 963/1000, Loss: 1.8282676647816385\nEpoch 964/1000, Loss: 1.8280145829277379\nEpoch 965/1000, Loss: 1.8277621109570776\nEpoch 966/1000, Loss: 1.8275102589811598\nEpoch 967/1000, Loss: 1.8272589902792657\nEpoch 968/1000, Loss: 1.8270082926111562\nEpoch 969/1000, Loss: 1.8267581505434853\nEpoch 970/1000, Loss: 1.8265086790280682\nEpoch 971/1000, Loss: 1.826259761516537\nEpoch 972/1000, Loss: 1.8260113894939423\nEpoch 973/1000, Loss: 1.8257636805730206\nEpoch 974/1000, Loss: 1.8255165320421969\nEpoch 975/1000, Loss: 1.8252699034554618\nEpoch 976/1000, Loss: 1.825023936373847\nEpoch 977/1000, Loss: 1.8247785190386432\nEpoch 978/1000, Loss: 1.8245337174407072\nEpoch 979/1000, Loss: 1.8242894592029708\nEpoch 980/1000, Loss: 1.8240457751921244\nEpoch 981/1000, Loss: 1.8238026845668043\nEpoch 982/1000, Loss: 1.8235601436878954\nEpoch 983/1000, Loss: 1.8233181796967983\nEpoch 984/1000, Loss: 1.8230768160096236\nEpoch 985/1000, Loss: 1.8228359472538744\nEpoch 986/1000, Loss: 1.822595729891743\nEpoch 987/1000, Loss: 1.8223560968680041\nEpoch 988/1000, Loss: 1.8221169646297182\nEpoch 989/1000, Loss: 1.8218783810734749\nEpoch 990/1000, Loss: 1.8216403696153844\nEpoch 991/1000, Loss: 1.8214029467531614\nEpoch 992/1000, Loss: 1.8211660789591926\nEpoch 993/1000, Loss: 1.820929764104741\nEpoch 994/1000, Loss: 1.8206940325243133\nEpoch 995/1000, Loss: 1.8204588166304998\nEpoch 996/1000, Loss: 1.8202241925256593\nEpoch 997/1000, Loss: 1.8199901559523173\nEpoch 998/1000, Loss: 1.8197565877011843\nEpoch 999/1000, Loss: 1.819523652749402\nEpoch 1000/1000, Loss: 1.8192912366773402\n['ayé', 'orún', 'náà']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(predict(model, \"káàkiri\", vocab, 3, device))","metadata":{"execution":{"iopub.status.busy":"2024-08-31T11:28:43.402367Z","iopub.execute_input":"2024-08-31T11:28:43.403140Z","iopub.status.idle":"2024-08-31T11:28:43.409880Z","shell.execute_reply.started":"2024-08-31T11:28:43.403088Z","shell.execute_reply":"2024-08-31T11:28:43.408752Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['ayé', 'yípo', 'orún']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(predict(model, \"gbogbo\", vocab, 3, device))","metadata":{"execution":{"iopub.status.busy":"2024-08-31T11:29:56.531242Z","iopub.execute_input":"2024-08-31T11:29:56.531734Z","iopub.status.idle":"2024-08-31T11:29:56.538700Z","shell.execute_reply.started":"2024-08-31T11:29:56.531688Z","shell.execute_reply":"2024-08-31T11:29:56.537599Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['sí', 'ayé', 'ró']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}